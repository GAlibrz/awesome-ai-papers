# ðŸ“š AI Papers Iâ€™m Reading

A curated list of papers Iâ€™ve read (or plan to read) in **AI, LLMs, Reinforcement Learning, Federated Learning, Agentic AI, Computer Vision, and Meta Learning**, with short notes for context.  
This serves as my personal learning log and a reference for others interested in applied AI research.  

---

## ðŸ”¹ Foundations
- [Attention is All You Need (Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762)  
  â†’ Introduced the Transformer architecture, foundation of modern NLP and LLMs.  
- [Deep Residual Learning for Image Recognition (He et al., 2015)](https://arxiv.org/abs/1512.03385)  
  â†’ ResNet, enabling very deep networks with skip connections.  
- [Adam: A Method for Stochastic Optimization (Kingma & Ba, 2015)](https://arxiv.org/abs/1412.6980)  
  â†’ Optimizer widely used for training neural networks.  

---

## ðŸ”¹ Computer Vision
- [ImageNet Classification with Deep Convolutional Networks (Krizhevsky et al., 2012)](https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html)  
  â†’ AlexNet, breakthrough CNN model that kicked off the DL revolution.  
- [Mask R-CNN (He et al., 2017)](https://arxiv.org/abs/1703.06870)  
  â†’ Instance segmentation framework, standard in detection pipelines.  
- [An Image is Worth 16x16 Words: Transformers for Image Recognition (Dosovitskiy et al., 2020)](https://arxiv.org/abs/2010.11929)  
  â†’ Vision Transformer (ViT), applying transformer architectures to vision tasks.  
-[Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)
  â†’ ResNet, introducing residual connections to enable training of very deep neural networks.

---

## ðŸ”¹ Large Language Models
- [BERT: Pre-training of Deep Bidirectional Transformers (Devlin et al., 2018)](https://arxiv.org/abs/1810.04805)  
  â†’ Introduced bidirectional transformer pretraining, foundational for NLP.  
- [Retrieval-Augmented Generation (Lewis et al., 2020)](https://arxiv.org/abs/2005.11401)  
  â†’ RAG, combining retrieval with generation for enriched context.  
- [AutoGen: Enabling Next-Gen LLM Applications (Microsoft, 2023)](https://arxiv.org/abs/2308.08155)  
  â†’ Multi-agent orchestration with LLMs.  

---

## ðŸ”¹ Reinforcement Learning
- [Human-level control through deep reinforcement learning (Mnih et al., 2015)](https://www.nature.com/articles/nature14236)  
  â†’ Deep Q-Network (DQN), combining deep learning with RL.  
- [Proximal Policy Optimization (PPO) (Schulman et al., 2017)](https://arxiv.org/abs/1707.06347)  
  â†’ Simple, stable, and efficient policy optimization.  
- [REINFORCE: Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning (Williams, 1992)](https://www.jmlr.org/papers/volume4/williams02a/williams02a.pdf)  
  â†’ Introduced REINFORCE algorithm, the foundation of policy gradient methods.
- [Trust Region Policy Optimization (TRPO, Schulman et al., 2015)](https://arxiv.org/abs/1502.05477)  
  â†’ Introduced KL-constrained updates for stable policy learning, precursor to PPO.
- [Generalized Advantage Estimation (GAE, Schulman et al., 2015)](https://arxiv.org/abs/1506.02438)  
  â†’ Variance reduction technique for actor-critic methods, widely used with PPO.

- [Actor-Critic Algorithms (Konda & Tsitsiklis, 1999)](https://proceedings.neurips.cc/paper_files/paper/1999/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf)  
  â†’ Theoretical foundation of the actor-critic architecture in policy-based RL.
---

## ðŸ”¹ Federated Learning
- [Communication-Efficient Learning of Deep Networks from Decentralized Data (McMahan et al., 2017)](https://arxiv.org/abs/1602.05629)  
  â†’ FedAvg, foundational algorithm in federated learning.  
- [Advances and Open Problems in Federated Learning (Kairouz et al., 2019)](https://arxiv.org/abs/1912.04977)  
  â†’ Comprehensive survey of challenges and directions.  

---

## ðŸ”¹ Agentic AI & Multi-Agent Systems
- [Voyager: An Open-Ended Embodied Agent (Wang et al., 2023)](https://arxiv.org/abs/2305.16291)  
  â†’ LLM-powered agent for continual learning in open worlds.  
- [AutoGen: Enabling Next-Gen LLM Applications (Microsoft, 2023)](https://arxiv.org/abs/2308.08155)  
  â†’ Framework for multi-agent orchestration of LLMs.  

---

## ðŸ”¹ Meta-Learning
- [Learning to Learn by Gradient Descent by Gradient Descent (Andrychowicz et al., 2016)](https://arxiv.org/abs/1606.04474)  
  â†’ Introduced meta-learning where an optimizer itself is learned through gradient descent.  

## ðŸ“Œ Goal
This repository is my personal learning log to stay engaged with key AI research.  
Notes and insights will be added as I progress. Contributions and suggestions are welcome!